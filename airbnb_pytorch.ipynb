{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "airbnb_pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO5i18U7QzFcxuQAwcguGA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emadphysics/Amsterdam_Airbnb_predictive_models/blob/main/airbnb_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmKvYRHtIFpY",
        "outputId": "24a2ab47-b1ea-41ad-df7b-82ed2c93be11"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from datetime import date\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import os\r\n",
        "import re\r\n",
        "from sklearn.feature_selection import *\r\n",
        "from sklearn.linear_model import *\r\n",
        "from sklearn.neighbors import *\r\n",
        "from sklearn.svm import *\r\n",
        "from sklearn.neighbors import *\r\n",
        "from sklearn.tree import *\r\n",
        "from sklearn.preprocessing import *\r\n",
        "from xgboost import *\r\n",
        "from sklearn.metrics import *\r\n",
        "from geopy.distance import great_circle\r\n",
        "# Geographical analysis\r\n",
        "import json # library to handle JSON files\r\n",
        "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\r\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\r\n",
        "import requests\r\n",
        "import descartes\r\n",
        "import math\r\n",
        "\r\n",
        "\r\n",
        "print('Libraries imported.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Libraries imported.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTwfiEuAIrvA",
        "outputId": "b4476dcc-df50-427e-d37e-8742e5a09829"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xReQ5oEbIXUS",
        "outputId": "1b5055ee-5f06-4773-c6ae-af2b11ebfcc7"
      },
      "source": [
        "df=pd.read_csv('/content/gdrive/My Drive/listingss.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (61,62,94) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkC4eYVrI5tv",
        "outputId": "ba663a83-c155-44bf-a61e-cd38d3979365"
      },
      "source": [
        "print(f'the numer of observations are {len(df)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the numer of observations are 20001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqvhO0prKVq3"
      },
      "source": [
        "categoricals = [var for var in df.columns if df[var].dtype=='object']\r\n",
        "numerics = [var for var in df.columns if (df[var].dtype=='int64')|(df[var].dtype=='float64')]\r\n",
        "dates=[var for var in df.columns if df[var].dtype=='datetime64[ns]']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuHt5Gj_KURe"
      },
      "source": [
        "#pandas data types: numeric(float,integer),object(string),category,Boolean,date\r\n",
        "one_hot_col_names = ['host_id',  'host_location', 'host_response_time','host_is_superhost','host_neighbourhood','host_has_profile_pic','host_identity_verified',\r\n",
        "           'neighbourhood','neighbourhood_cleansed','neighbourhood_group_cleansed', 'zipcode', 'is_location_exact', 'property_type', 'room_type', 'bed_type', 'has_availability', 'requires_license', 'instant_bookable', \r\n",
        "           'is_business_travel_ready', 'cancellation_policy', 'cancellation_policy','require_guest_profile_picture', 'require_guest_phone_verification', 'calendar_updated']\r\n",
        "\r\n",
        "text_cols = ['name', 'summary', 'space', 'description', 'neighborhood_overview', 'notes', 'transit', 'access', 'interaction', 'house_rules', 'host_name', 'host_about']\r\n",
        "\r\n",
        "features = ['host_listings_count', 'host_total_listings_count', 'latitude', 'longitude', \r\n",
        "      'accommodates', 'bathrooms', 'bedrooms', 'beds', 'square_feet',     \r\n",
        "      'guests_included', 'minimum_nights', 'maximum_nights', 'availability_30', 'availability_60', \r\n",
        "      'availability_90', 'availability_365', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy', \r\n",
        "      'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', \r\n",
        "      'review_scores_value', 'calculated_host_listings_count', 'reviews_per_month']\r\n",
        " \r\n",
        "price_features = ['security_deposit', 'cleaning_fee', 'extra_people','price'] \r\n",
        "\r\n",
        "\r\n",
        "date_cols = ['host_since', 'first_review', 'last_review']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFLhX1E5JWSl"
      },
      "source": [
        "def host_verification(cols):\r\n",
        "    possible_words = {}\r\n",
        "    i = 0\r\n",
        "    for col in cols:\r\n",
        "        words = col.split()\r\n",
        "        for w in words:\r\n",
        "            wr = re.sub(r'\\W+', '', w)\r\n",
        "            if wr != '' and wr not in possible_words:\r\n",
        "                possible_words[wr] = i\r\n",
        "                i += 1\r\n",
        "    l = len(possible_words)\r\n",
        "\r\n",
        "    new_cols = np.zeros((cols.shape[0], l))\r\n",
        "    for i, col in enumerate(cols):\r\n",
        "        words = col.split()\r\n",
        "        arr = np.zeros(l)\r\n",
        "        for w in words:\r\n",
        "            wr = re.sub(r'\\W+', '', w)\r\n",
        "            if wr != '':\r\n",
        "                arr[possible_words[wr]] = 1\r\n",
        "        new_cols[i] = arr\r\n",
        "    return new_cols\r\n",
        "\r\n",
        "def amenities(cols):\r\n",
        "    dic = {}\r\n",
        "    i = 0\r\n",
        "    for col in cols:\r\n",
        "        arr = col.split(',')\r\n",
        "        for a in arr:\r\n",
        "            ar = re.sub(r'\\W+', '', a)\r\n",
        "            if len(ar) > 0:\r\n",
        "                if ar not in dic:\r\n",
        "                    dic[ar] = i\r\n",
        "                    i += 1\r\n",
        "    \r\n",
        "    l = len(dic)\r\n",
        "    new_cols = np.zeros((cols.shape[0], l))\r\n",
        "    for i, col in enumerate(cols):\r\n",
        "        words = col.split(',')\r\n",
        "        arr = np.zeros(l)\r\n",
        "        for w in words:\r\n",
        "            wr = re.sub(r'\\W+', '', w)\r\n",
        "            if wr != '':\r\n",
        "                arr[dic[wr]] = 1\r\n",
        "        new_cols[i] = arr\r\n",
        "    return new_cols\r\n",
        "\r\n",
        "\r\n",
        "def one_hot(arr):\r\n",
        "    \r\n",
        "    label_encoder = LabelEncoder()\r\n",
        "    integer_encoded = label_encoder.fit_transform(arr)\r\n",
        "    onehot_encoder = OneHotEncoder(sparse=False)\r\n",
        "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\r\n",
        "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\r\n",
        "    return onehot_encoded\r\n",
        "\r\n",
        "one_hot_col_names = ['host_response_time','host_is_superhost','host_has_profile_pic','host_identity_verified',\r\n",
        "           'neighbourhood_cleansed','neighbourhood_group_cleansed', 'zipcode', 'is_location_exact', 'property_type', 'room_type', 'bed_type', 'has_availability', 'requires_license', 'instant_bookable', \r\n",
        "           'is_business_travel_ready', 'cancellation_policy','require_guest_profile_picture', 'require_guest_phone_verification','calendar_updated']\r\n",
        "one_hot_dict = {}\r\n",
        "for i in one_hot_col_names:\r\n",
        "    one_hot_dict[i] = one_hot(np.array(df[i].fillna(\"\"), dtype=str))\r\n",
        "one_hot_dict['host_verifications'] = host_verification(df['host_verifications'])\r\n",
        "one_hot_dict['amenities'] = amenities(df['amenities'])\r\n",
        "ont_hot_list = []\r\n",
        "\r\n",
        "for i in one_hot_dict.keys():\r\n",
        "    if 1<one_hot_dict[i].shape[1]<400:\r\n",
        "        \r\n",
        "        ont_hot_list.append(one_hot_dict[i])\r\n",
        "#        print(i,one_hot_dict[i].shape[1])\r\n",
        "onehot_variables = np.concatenate(ont_hot_list, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecn_Z-VjKx1P",
        "outputId": "232cec2f-e055-4696-e5a1-dec9ef576bc8"
      },
      "source": [
        "hot_cat_variables=pd.DataFrame(onehot_variables)\r\n",
        "hot_cat_variables.isnull().sum().sum()\r\n",
        "hot_cat_variables.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20001, 325)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfYJ_l16KHgH"
      },
      "source": [
        "def check_nan(cols):\r\n",
        "    for col in cols:\r\n",
        "        if np.isnan(col):\r\n",
        "            return True\r\n",
        "    return False\r\n",
        "\r\n",
        "def clean_host_response_rate(host_response_rate, num_data):\r\n",
        "    total = 0\r\n",
        "    count = 0\r\n",
        "    for col in host_response_rate:\r\n",
        "        if not isinstance(col, float):\r\n",
        "            total += float(col.strip('%'))\r\n",
        "            count += 1\r\n",
        "\r\n",
        "    arr = np.zeros(num_data)\r\n",
        "    mean = total / count\r\n",
        "    for i, col in enumerate(host_response_rate):\r\n",
        "        if not isinstance(col, float):\r\n",
        "            arr[i] += float(col.strip('%'))\r\n",
        "        else:\r\n",
        "            assert(math.isnan(col))\r\n",
        "            arr[i] = mean\r\n",
        "    return arr\r\n",
        "\r\n",
        "def clean_price(price, num_data):\r\n",
        "    arr = np.zeros(num_data)\r\n",
        "    for i, col in enumerate(price):\r\n",
        "        if not isinstance(col, float):\r\n",
        "            arr[i] += float(col.strip('$').replace(',', ''))\r\n",
        "        else:\r\n",
        "            assert(math.isnan(col))\r\n",
        "            arr[i] = 0\r\n",
        "    return arr\r\n",
        "\r\n",
        "def to_np_array_fill_NA_mean(cols):\r\n",
        "    return np.array(cols.fillna(np.nanmean(np.array(cols))))\r\n",
        "\r\n",
        "\r\n",
        "num_data = df.shape[0]\r\n",
        "arr = np.zeros((len(features) + len(price_features) + 1, num_data))\r\n",
        "\r\n",
        "host_response_rate = clean_host_response_rate(df['host_response_rate'], num_data)\r\n",
        "arr[0] = host_response_rate\r\n",
        "i = 0\r\n",
        "for feature in features:\r\n",
        "    i += 1\r\n",
        "    if check_nan(df[feature]):\r\n",
        "        arr[i] = to_np_array_fill_NA_mean(df[feature])\r\n",
        "    else:\r\n",
        "        arr[i] = np.array(df[feature])\r\n",
        "    \r\n",
        "for feature in price_features:\r\n",
        "    i += 1\r\n",
        "    arr[i] = clean_price(df[feature], num_data)\r\n",
        "\r\n",
        "target = arr[-1]\r\n",
        "numeric_variables = arr[:-1].T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRi_aEJXKfk5",
        "outputId": "e9b54f05-a5d8-41b8-bc98-9106dc93fb17"
      },
      "source": [
        "numeric_variables=pd.DataFrame(numeric_variables)\r\n",
        "numeric_variables.isnull().sum()\\\r\n",
        "                          .sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IuM9g2rKodx"
      },
      "source": [
        "inde_variables=np.concatenate((numeric_variables,hot_cat_variables),axis=1)\r\n",
        "inde_variables=pd.DataFrame(inde_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IZojGNrLoye",
        "outputId": "0297b09c-c3ea-48a8-b42c-d828f80f1de1"
      },
      "source": [
        "inde_variables.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBr61uC6LHzq"
      },
      "source": [
        "mean = np.mean(inde_variables, axis = 0)\r\n",
        "std = np.std(inde_variables, axis = 0)\r\n",
        "inde_variables=(inde_variables-mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_dX-lUnMLES",
        "outputId": "1fef9e64-a478-4029-9c2f-da0c06e8970f"
      },
      "source": [
        "inde_variables.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20001, 355)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObUHuQ2WMDID"
      },
      "source": [
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.optim as optim\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import copy\r\n",
        "import torch.utils.data as data\r\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcaNVynmMI0H"
      },
      "source": [
        "class NN229(nn.Module):\r\n",
        "    def __init__(self, input_size=355, hidden_size1=128, hidden_size2=512, hidden_size3=64, output_size=1, drop_prob=0.05):\r\n",
        "        super(NN229, self).__init__()\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\r\n",
        "        self.W1 = nn.Linear(input_size, hidden_size1)\r\n",
        "        self.W2 = nn.Linear(hidden_size1, hidden_size2)\r\n",
        "        self.W3 = nn.Linear(hidden_size2, hidden_size3)\r\n",
        "        self.W4 = nn.Linear(hidden_size3, output_size)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        hidden1 = self.dropout(self.relu(self.W1(x)))\r\n",
        "        hidden2 = self.dropout(self.relu(self.W2(hidden1)))\r\n",
        "        hidden3 = self.dropout(self.relu(self.W3(hidden2)))\r\n",
        "        out = self.W4(hidden3)\r\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rrFk3TAMQup"
      },
      "source": [
        "class AirBnb(data.Dataset):\r\n",
        "    def __init__(self, train_path, label_path):\r\n",
        "        super(AirBnb, self).__init__()\r\n",
        "\r\n",
        "        self.x = torch.from_numpy(train_path).float()\r\n",
        "        self.y = torch.from_numpy(label_path).float()\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        x = self.x[idx]\r\n",
        "        y = self.y[idx]\r\n",
        "        \r\n",
        "        return x, y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.x.shape[0]\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvuFWfevSKBU"
      },
      "source": [
        "\r\n",
        "class CSVDataset(data.Dataset):\r\n",
        "\r\n",
        "  def __init__(self, train_path, label_path):\r\n",
        "\r\n",
        "        super(CSVDataset, self).__init__()\r\n",
        "        self.x = torch.from_numpy(train_path).float()\r\n",
        "        self.y = torch.from_numpy(label_path).float()\r\n",
        "        self.y = self.y.reshape((len(self.y), 1))\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.x)\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    return [self.x[idx], self.y[idx]]\r\n",
        "  def get_splits(self, n_test=0.33):\r\n",
        "    test_size = round(n_test * len(self.x))\r\n",
        "    train_size = len(self.x) - test_size\r\n",
        "    return data.random_split(self, [train_size, test_size])        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75rTImCdMlmA"
      },
      "source": [
        "def load_model(model, optimizer, checkpoint_path, model_only = False):\r\n",
        "    ckpt_dict = torch.load(checkpoint_path, map_location=\"cuda:0\")\r\n",
        "\r\n",
        "    model.load_state_dict(ckpt_dict['state_dict'])\r\n",
        "    if not model_only:\r\n",
        "        optimizer.load_state_dict(ckpt_dict['optimizer'])\r\n",
        "        epoch = ckpt_dict['epoch']\r\n",
        "        val_loss = ckpt_dict['val_loss']\r\n",
        "    else:\r\n",
        "        epoch = None\r\n",
        "        val_loss = None\r\n",
        "    return model, optimizer, epoch, val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SwEoqjbWGfJ",
        "outputId": "090fc5d1-7327-43a7-e3da-848e7d8dd700"
      },
      "source": [
        "np.log(target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.07753744, 4.60517019, 4.82831374, ..., 5.01063529, 5.85793315,\n",
              "       5.59842196])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9SxBsa_MoWe"
      },
      "source": [
        "def train(model, optimizer, loss_fn, epoch = 0):\r\n",
        "    train_dataset = CSVDataset(inde_variables.to_numpy(), target)\r\n",
        "    train, test = train_dataset.get_splits()\r\n",
        "    train_loader = data.DataLoader(train,\r\n",
        "                                  batch_size=batch_size,\r\n",
        "                                  shuffle=True)\r\n",
        "    dev_loader = data.DataLoader(test,\r\n",
        "                                batch_size=batch_size,\r\n",
        "                                shuffle=True)\r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    step = 0\r\n",
        "    best_model = NN229()\r\n",
        "    best_epoch = 0\r\n",
        "    best_val_loss = None\r\n",
        "    while epoch < max_epoch:\r\n",
        "        epoch += 1\r\n",
        "        stats = []\r\n",
        "        with torch.enable_grad():\r\n",
        "            for x, y in train_loader:\r\n",
        "                step += 1\r\n",
        "                # print (x)\r\n",
        "                # print (y)\r\n",
        "                # break\r\n",
        "                x = x.cuda()\r\n",
        "                y = y.cuda()\r\n",
        "                optimizer.zero_grad()\r\n",
        "                pred = model(x).reshape(-1)\r\n",
        "                loss = loss_fn(pred, y)\r\n",
        "                loss_val = loss.item()\r\n",
        "                loss.backward()\r\n",
        "                optimizer.step()\r\n",
        "                stats.append(loss_val)\r\n",
        "                # stats.append((epoch, step, loss_val))\r\n",
        "                # print (\"Epoch: \", epoch, \" Step: \", step, \" Loss: \", loss_val)\r\n",
        "        print (\"Train loss: \", sum(stats) / len(stats))\r\n",
        "        val_loss = evaluate(dev_loader, model)\r\n",
        "        if best_val_loss is None or best_val_loss > val_loss:\r\n",
        "            best_val_loss = val_loss\r\n",
        "            model.cpu()\r\n",
        "            best_model = copy.deepcopy(model)\r\n",
        "            model.cuda()\r\n",
        "            best_epoch = epoch\r\n",
        "        # print (evaluate(dev_loader, model))\r\n",
        "        \r\n",
        "    return best_model, best_epoch, best_val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDmivyoIM4i_"
      },
      "source": [
        "def evaluate(dev_loader, model):\r\n",
        "    model.eval()\r\n",
        "    stats = []\r\n",
        "    with torch.no_grad():\r\n",
        "        for x, y in dev_loader:\r\n",
        "            x = x.cuda()\r\n",
        "            y = y.cuda()\r\n",
        "            pred = model(x).reshape(-1)\r\n",
        "            loss_val = loss_fn(pred, y).item()\r\n",
        "            stats.append(loss_val)\r\n",
        "            # print (\"Loss: \", loss_val)\r\n",
        "    print (\"Val loss: \", sum(stats) / len(stats))\r\n",
        "    return sum(stats) / len(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E5lR1QHM4mE"
      },
      "source": [
        "lr = 1e-4\r\n",
        "weight_decay = 1e-5\r\n",
        "beta = (0.9, 0.999)\r\n",
        "max_epoch = 100\r\n",
        "batch_size = 64\r\n",
        "\r\n",
        "model = NN229().cuda()\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=beta)\r\n",
        "loss_fn = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2XAzZznM4qY",
        "outputId": "e4dd96f2-36a1-4ff4-c6d4-5334fad92db3"
      },
      "source": [
        "best_model, best_epoch, best_val_loss = train(model, optimizer, loss_fn, epoch = 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([25, 1])) that is different to the input size (torch.Size([25])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss:  68014.49023204985\n",
            "Val loss:  35401.672551081734\n",
            "Train loss:  47669.987432570684\n",
            "Val loss:  28291.954876239484\n",
            "Train loss:  46416.13604678199\n",
            "Val loss:  27948.086291973406\n",
            "Train loss:  46333.08009091332\n",
            "Val loss:  28103.012606107273\n",
            "Train loss:  46272.00659644717\n",
            "Val loss:  27834.81196946364\n",
            "Train loss:  46359.9581577846\n",
            "Val loss:  27794.064704308144\n",
            "Train loss:  46190.28921828497\n",
            "Val loss:  27769.424067570613\n",
            "Train loss:  46216.929271298366\n",
            "Val loss:  30421.354543832633\n",
            "Train loss:  46181.427563476565\n",
            "Val loss:  27703.999422513523\n",
            "Train loss:  46138.743073381695\n",
            "Val loss:  27706.84616323618\n",
            "Train loss:  46139.00078938802\n",
            "Val loss:  30356.88977285532\n",
            "Train loss:  46145.648843238465\n",
            "Val loss:  27746.72165621244\n",
            "Train loss:  46125.407893880205\n",
            "Val loss:  27741.699932391828\n",
            "Train loss:  46119.57061360677\n",
            "Val loss:  27793.257497934195\n",
            "Train loss:  46118.91447521391\n",
            "Val loss:  27752.09123347356\n",
            "Train loss:  46234.11735491071\n",
            "Val loss:  27802.720759465145\n",
            "Train loss:  46112.97265857515\n",
            "Val loss:  27904.161888709434\n",
            "Train loss:  46107.12628464472\n",
            "Val loss:  27694.376732459434\n",
            "Train loss:  46156.96800827753\n",
            "Val loss:  27789.660125732422\n",
            "Train loss:  46098.854736328125\n",
            "Val loss:  27913.111091026894\n",
            "Train loss:  46057.64225725446\n",
            "Val loss:  28307.207571176383\n",
            "Train loss:  46097.959001813615\n",
            "Val loss:  27745.517010028547\n",
            "Train loss:  46117.88002929687\n",
            "Val loss:  27721.22836773212\n",
            "Train loss:  46086.921350678946\n",
            "Val loss:  27711.28061617338\n",
            "Train loss:  46089.90958658854\n",
            "Val loss:  27741.02340932993\n",
            "Train loss:  46100.98425874256\n",
            "Val loss:  30356.122666579027\n",
            "Train loss:  46074.31770949591\n",
            "Val loss:  27655.731746967023\n",
            "Train loss:  46124.207779947916\n",
            "Val loss:  27804.186166616586\n",
            "Train loss:  46082.15463634673\n",
            "Val loss:  27684.265343299277\n",
            "Train loss:  46081.08661993118\n",
            "Val loss:  27693.762568547176\n",
            "Train loss:  46110.93073497954\n",
            "Val loss:  27680.14206167368\n",
            "Train loss:  46056.748182896205\n",
            "Val loss:  27677.445087139422\n",
            "Train loss:  46072.271701776415\n",
            "Val loss:  28394.20701951247\n",
            "Train loss:  46084.17601609003\n",
            "Val loss:  27684.068969726562\n",
            "Train loss:  55146.51544480097\n",
            "Val loss:  27672.136415921726\n",
            "Train loss:  46070.84659946986\n",
            "Val loss:  27763.450214092547\n",
            "Train loss:  46114.40060105097\n",
            "Val loss:  27664.991786076473\n",
            "Train loss:  46113.53148484003\n",
            "Val loss:  27657.09431340144\n",
            "Train loss:  46070.8599062965\n",
            "Val loss:  27735.698120117188\n",
            "Train loss:  46081.746666899184\n",
            "Val loss:  27661.552112285906\n",
            "Train loss:  46102.260134161086\n",
            "Val loss:  27682.934018648586\n",
            "Train loss:  46063.15468517485\n",
            "Val loss:  27664.89033156175\n",
            "Train loss:  46056.73315894717\n",
            "Val loss:  27700.36111684946\n",
            "Train loss:  46154.81727120536\n",
            "Val loss:  28069.638162466195\n",
            "Train loss:  46064.4858014788\n",
            "Val loss:  27687.754173865684\n",
            "Train loss:  46064.53990187872\n",
            "Val loss:  27655.230032113883\n",
            "Train loss:  46097.85264253162\n",
            "Val loss:  27669.643376277043\n",
            "Train loss:  46062.49197707403\n",
            "Val loss:  28878.095123291016\n",
            "Train loss:  48809.06289178757\n",
            "Val loss:  27656.852142333984\n",
            "Train loss:  46087.67579985119\n",
            "Val loss:  27649.813608022836\n",
            "Train loss:  46082.45280645461\n",
            "Val loss:  27673.19396033654\n",
            "Train loss:  46058.888174293155\n",
            "Val loss:  27681.646615835336\n",
            "Train loss:  46069.9608328683\n",
            "Val loss:  27665.612626295824\n",
            "Train loss:  46059.80949358259\n",
            "Val loss:  27640.849919245793\n",
            "Train loss:  46082.04990234375\n",
            "Val loss:  27785.744241567758\n",
            "Train loss:  46059.59813174293\n",
            "Val loss:  28409.25119957557\n",
            "Train loss:  46073.500397600445\n",
            "Val loss:  27727.78260216346\n",
            "Train loss:  55132.689934430804\n",
            "Val loss:  27685.973041240984\n",
            "Train loss:  46054.36003301711\n",
            "Val loss:  27669.12972318209\n",
            "Train loss:  46062.347841099334\n",
            "Val loss:  27729.48012601412\n",
            "Train loss:  46062.149512881326\n",
            "Val loss:  30304.845177283652\n",
            "Train loss:  46155.48791155134\n",
            "Val loss:  27642.734569843\n",
            "Train loss:  46089.96072939918\n",
            "Val loss:  27703.05389873798\n",
            "Train loss:  46052.798036411834\n",
            "Val loss:  27688.26137601412\n",
            "Train loss:  46068.63589564732\n",
            "Val loss:  27793.000769981973\n",
            "Train loss:  46065.01847446986\n",
            "Val loss:  27641.95020705003\n",
            "Train loss:  46102.195826357885\n",
            "Val loss:  27822.039794921875\n",
            "Train loss:  46069.372839936754\n",
            "Val loss:  27722.82823768029\n",
            "Train loss:  46051.65311686198\n",
            "Val loss:  27662.341954157902\n",
            "Train loss:  46058.88141043527\n",
            "Val loss:  27762.864100529598\n",
            "Train loss:  46072.727540225074\n",
            "Val loss:  27663.748755821816\n",
            "Train loss:  46051.678331938245\n",
            "Val loss:  28377.736837533805\n",
            "Train loss:  46051.91812686012\n",
            "Val loss:  27647.421438363883\n",
            "Train loss:  46062.990143694195\n",
            "Val loss:  27654.76111778846\n",
            "Train loss:  46055.24193173363\n",
            "Val loss:  27742.2734774076\n",
            "Train loss:  46065.635085332964\n",
            "Val loss:  27765.00931959886\n",
            "Train loss:  46049.90898786272\n",
            "Val loss:  27675.581399770883\n",
            "Train loss:  46060.28251953125\n",
            "Val loss:  27675.840839092547\n",
            "Train loss:  46052.30663481213\n",
            "Val loss:  27637.565969613883\n",
            "Train loss:  46098.26823265439\n",
            "Val loss:  27662.328127347508\n",
            "Train loss:  46071.46564476377\n",
            "Val loss:  27640.08705491286\n",
            "Train loss:  46207.199209449405\n",
            "Val loss:  27751.241807204027\n",
            "Train loss:  46052.79894438244\n",
            "Val loss:  28060.465754582332\n",
            "Train loss:  46053.148082914806\n",
            "Val loss:  27688.757751464844\n",
            "Train loss:  46041.761070033484\n",
            "Val loss:  27751.031198354867\n",
            "Train loss:  46049.73108026414\n",
            "Val loss:  27704.927563007062\n",
            "Train loss:  46051.40098237537\n",
            "Val loss:  27657.848337026742\n",
            "Train loss:  46029.09210146949\n",
            "Val loss:  27766.35692889874\n",
            "Train loss:  46079.17290969122\n",
            "Val loss:  27710.851769080527\n",
            "Train loss:  46147.78131975447\n",
            "Val loss:  27645.069056584285\n",
            "Train loss:  46059.5220156715\n",
            "Val loss:  27642.791802039512\n",
            "Train loss:  46110.60864141555\n",
            "Val loss:  27699.809664212742\n",
            "Train loss:  46072.510481770834\n",
            "Val loss:  27641.081044123723\n",
            "Train loss:  46063.52380138579\n",
            "Val loss:  27829.877312293418\n",
            "Train loss:  46033.68952287947\n",
            "Val loss:  27742.937598595254\n",
            "Train loss:  46110.34655645461\n",
            "Val loss:  27667.9050386869\n",
            "Train loss:  46093.50830426897\n",
            "Val loss:  27644.448148287258\n",
            "Train loss:  46075.39552408854\n",
            "Val loss:  27661.741537240836\n",
            "Train loss:  46197.71840122768\n",
            "Val loss:  27668.130950927734\n",
            "Train loss:  46083.27601376488\n",
            "Val loss:  27704.13420222356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6JW_90jZ0Kb"
      },
      "source": [
        "train_dataset = CSVDataset(inde_variables.to_numpy(), target)\r\n",
        "train, test = train_dataset.get_splits()\r\n",
        "dev_loader = data.DataLoader(test,\r\n",
        "                                shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF7bSxRbdiyi",
        "outputId": "9a228423-42c1-4827-f17f-1cdf0344117b"
      },
      "source": [
        "y_truth_list = []\r\n",
        "\r\n",
        "for _, y_truth in dev_loader:\r\n",
        "  y_truth_list.append(y_truth[0][0].cpu().numpy())\r\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_truth_list]\r\n",
        "y_t=np.array(y_truth_list)\r\n",
        "y_t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 60., 165., 295., ..., 125., 120., 130.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXt1kU8MZj3c"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "y_pred_list = []\r\n",
        "with torch.no_grad():\r\n",
        "    model.eval()\r\n",
        "    for X_batch, _ in dev_loader:\r\n",
        "        X_batch = X_batch.to(device)\r\n",
        "        y_test_pred = model(X_batch)\r\n",
        "        y_pred_list.append(y_test_pred.cpu().numpy())\r\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\r\n",
        "y_p=np.array(y_pred_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s09ouH0NdKWG",
        "outputId": "f205b19f-4fbd-4bba-f7ff-993553b57e4f"
      },
      "source": [
        "y_p"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([162.69920349, 154.38798523, 165.96896362, ..., 176.99777222,\n",
              "       166.68391418, 162.90364075])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3PCVRe_gqQ-",
        "outputId": "3d1bffe5-9faa-4b65-eebe-04dc9b5adf43"
      },
      "source": [
        "import sklearn.metrics\r\n",
        "sklearn.metrics.r2_score(y_t, y_p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0017828930730376946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    }
  ]
}